# 🎞 Full Educational Guide: Containerizing a Full-Stack App (Client + Server + ML API)

This guide walks through each step required to containerize a full-stack application consisting of a React client, a Node.js server, and a Flask-based ML API, based on your shared project structure. Each code block is followed by a description of what each **line** does to clarify its purpose.

---

## 🧱 Project Structure Overview

project/
├── client/              # React app (Vite or CRA)
│   ├── .env             # Frontend environment variables (e.g., VITE_*)
│   ├── nginx.conf       # Custom Nginx config
│   ├── Dockerfile       # For client container
│   └── ...
├── server/              # Node.js Express backend
│   ├── routes/
│   ├── Dockerfile
│   ├── server.js
│   ├── package.json
│   └── ...
├── ML_API/              # Python Flask ML service
│   ├── model/
│   ├── Dockerfile
│   ├── app.py
│   ├── requirements.txt
│   ├── .env             # AWS keys & environment config
│   └── ...
├── docker-compose.yml   # Compose configuration
└── README.md

---

## 🐳 Step 0: Installing Docker & Docker Compose

Install Docker Desktop for Windows, Mac, or Linux:
- 📦 Download: https://www.docker.com/products/docker-desktop
- 🧰 It includes both `docker` and `docker-compose`

Verify installation:
    docker -v
    docker-compose -v

> This ensures you have the Docker engine and Docker Compose CLI working properly.

---

## 🐫 Step-by-Step: Containerizing Each Service

### ✅ 1. Client (React App + Nginx)

Dockerfile (client/Dockerfile)

# --- build phase ---
FROM node:20 as build                # Use Node.js 20 to build React app
WORKDIR /app                        # Set working directory inside container
COPY package*.json ./              # Copy dependency files
RUN npm install                    # Install dependencies
COPY . .                            # Copy all app files
RUN npm run build                  # Create production build in /app/dist or /app/build

# --- production nginx phase ---
FROM nginx:alpine                   # Use lightweight Nginx base image
COPY --from=build /app/dist /usr/share/nginx/html  # Copy build files to Nginx folder
COPY nginx.conf /etc/nginx/conf.d/default.conf     # Replace default Nginx config
EXPOSE 80                           # Expose port 80 for HTTP
CMD ["nginx", "-g", "daemon off;"]    # Run Nginx in foreground

nginx.conf (client/nginx.conf)

server {
    listen 80;  # Web server listens on port 80

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
        try_files $uri $uri/ /index.html;  # React SPA fallback routing
    }

    location /api/ {
        proxy_pass http://server:4000/api/;  # Forward API calls to backend
    }
}

---

### ✅ 2. Server (Node.js Backend)

Dockerfile (server/Dockerfile)

FROM node:20                    # Use Node.js base image
WORKDIR /app                    # Set working directory
COPY package*.json ./          # Copy dependency files
RUN npm install                # Install dependencies
COPY . .                        # Copy source files
EXPOSE 4000                    # Expose backend port
CMD ["node", "server.js"]       # Start the server

Make sure your server listens on all interfaces:
    app.listen(4000, '0.0.0.0');

---

### ✅ 3. ML API (Flask + XGBoost + SES)

Dockerfile (ML_API/Dockerfile)

FROM python:3.11-slim                         # Lightweight Python base
WORKDIR /app                                  # Set working directory
COPY requirements.txt .                       # Copy dependency file
RUN pip install --no-cache-dir -r requirements.txt  # Install Python packages
COPY . .                                      # Copy app files
ENV PYTHONUNBUFFERED=1                        # Output logs in real time
EXPOSE 5000                                   # Expose Flask API port
CMD ["python", "app.py"]                        # Start Flask app

requirements.txt (ML_API/requirements.txt)

flask                   # Web framework
flask_cors              # Handle CORS
xgboost                 # ML model
numpy                   # ML dependency
boto3                   # AWS SDK
dotenv                  # Load environment variables

---

## 🧹 Docker Compose File

docker-compose.yml

version: "3.9"
services:
  client:
    build: ./client                       # Build client from client/Dockerfile
    ports:
      - "3000:80"                         # Expose port 3000 on host
    depends_on:
      - server                            # Wait for server to start

  server:
    build: ./server
    ports:
      - "4000:4000"
    depends_on:
      - ml                                # Wait for ML service to start

  ml:
    build: ./ML_API
    ports:
      - "5000:5000"
    env_file:
      - ./ML_API/.env                     # Load environment variables securely

> This file defines and connects the services using Docker Compose.

---

## 🔁 Running the App

### 🔨 Initial Build
    docker-compose build

### 🚀 Start the App
    docker-compose up

### 🛑 Stop Everything
    docker-compose down

---

## 🔄 Updating After Code Changes

If you make code changes:

### For backend or ML:
    docker-compose build server  # or ml

### For frontend:
    docker-compose build client

Then:
    docker-compose up

Or simply:
    docker-compose up --build

> This will rebuild only changed parts thanks to Docker layer caching.

---

## 🧪 Testing ML API

curl -X POST http://localhost:4000/api/predict   -H "Content-Type: application/json"   -d '{
    "lecture_watch_pct": 80,
    "checklist_pct": 90,
    "attended_live_class": 1,
    "attended_group_discussion": 1,
    "qa_participation_pct": 75,
    "student_email": "test@example.com",
    "student_name": "Test Student"
  }'

> This tests if the backend and ML API are working together.

---

## ✅ Final Notes

- Keep `.env` files secure (in `.gitignore`).
- Ensure Nginx forwards `/api` requests properly.
- Confirm the correct build folder: `dist` for Vite or `build` for CRA.
- Docker greatly simplifies deployment and scaling.
